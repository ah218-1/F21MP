# -*- coding: utf-8 -*-
"""Parsing and Chunking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VS7YyAWOfINDEtANBnA3ZxQLlQnSn2R0
"""

import pandas as pd
import json
from transformers import AutoTokenizer
import json
import pickle


# Load the JSONL dataset
data_file = "super_scotus_sample_clean.jsonl"
cases = {}
gold_labels = {}

with open(data_file, "r") as f:
    for line in f:
        try:
            case = json.loads(line)
        except json.JSONDecodeError as e:
                print(f"Skipping malformed line: {e}")
                continue
        case_id = case.get("case_id") or case.get("id")
        justia_sections = case.get("justia_sections", {}) #judgments
        convos = case.get("convos", {})
        oyez_summary = case.get("oyez_summary", {})

        # Build the gold label text
        parts = []
        for key in ["Facts of the case", "Question", "Conclusion"]:
            section = oyez_summary.get(key)
            if isinstance(section, list):
                parts.append(" ".join(section))
        gold_label = "\n".join(parts).strip()

        if gold_label:
            gold_labels[case_id] = gold_label

        # Parse the convos
        utterances = []
        speaker_map = convos.get("speaker", {}) #oral transcripts

        if isinstance(convos, dict) and "utterances" in convos:
            outer = convos["utterances"]
            if (isinstance(outer, list) and len(outer) > 0 and isinstance(outer[0], list)
                    and all(isinstance(u, dict) for u in outer[0])):

                for utt in outer[0]:  # Flatten and parse
                    utt_id = utt.get("id")
                    speaker_id = utt.get("speaker_id")
                    text = utt.get("text", "")

                    speaker_meta = speaker_map.get(speaker_id, {}) if speaker_id else {}
                    speaker_type = speaker_meta.get("type")

                    # Debug this
                    if speaker_type is None:
                        print(f"[DEBUG] Missing speaker_type for: speaker_id={speaker_id}, utt_text={text[:50]}")

                    utterances.append({
                         "id": utt_id,
                         "text": text,
                         "speaker": speaker_id,
                         "side": speaker_meta.get("side"),
                         "speaker_type": speaker_type
                    })

            else:
                print(f"Warning: Unexpected 'utterances' format in case {case_id}. Skipping it.")


        # Store the parsed case
        cases[case_id] = {
            "justia_sections": justia_sections,
            "convos": utterances
        }

print(f"Loaded {len(cases)} cases from Super-SCOTUS dataset.")
print(f"Collected {len(gold_labels)} valid gold labels for ROUGE eval.")

# Example: Check structure for one case
example_id, example_case = next(iter(cases.items()))
print("Case ID:", example_id)
print("Justia sections:", list(example_case["justia_sections"].keys()))
print("First 2 transcript utterances:", example_case["convos"][:2])

# Example: Check one gold label
print("Example gold label:", gold_labels.get(example_id, "N/A"))

with open("data/gold_labels.pkl", "wb") as f:
    pickle.dump(gold_labels, f)

import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

# Custom splitter that prefers paragraph-level breaks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=100,
    separators=["\n\n", "\n", ".", "!", "?", " "]
)

justia_chunks = []

for case_id, case_data in cases.items():
    sections = case_data.get("justia_sections", {})
    if not sections:
        continue

    for section_name, section_text in sections.items():
        #Clean citations like [12]
        section_text = re.sub(r'\[\d+\]', '', section_text)

        #Extract justice name with regex
        sec_type = section_name
        justice = None
        match = re.search(r'Justice (\w+)', section_name)
        if match:
            justice = match.group(1)
        elif "Justice" in section_text:
            #try from body
            match_body = re.search(r'Justice (\w+)', section_text)
            if match_body:
                justice = match_body.group(1)

        #Normalize section type (e.g., Concurring / Dissenting)
        if "Concurring" in section_name:
            sec_type = "Concurring"
        elif "Dissent" in section_name:
            sec_type = "Dissenting"
        elif "Opinion" in section_name:
            sec_type = "Opinion"
        elif "Syllabus" in section_name:
            sec_type = "Syllabus"

        #Use chunker to split text into coherent pieces
        for chunk in text_splitter.split_text(section_text):
            doc = Document(
                page_content=chunk.strip(),
                metadata={
                    "case_id": case_id,
                    "section_type": sec_type,
                    "justice": justice
                }
            )
            justia_chunks.append(doc)

# Print summary
print(f"Total Justia chunks: {len(justia_chunks)} (from {len(cases)} cases)")
print("Sample chunk metadata:", justia_chunks[0].metadata)
print("Sample chunk text snippet:", justia_chunks[0], "...")

import pickle
import os

os.makedirs("data", exist_ok=True)
with open("data/justia_chunks.pkl", "wb") as f:
    pickle.dump(justia_chunks, f)



import re
import numpy as np
from langchain.schema import Document
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load semantic model
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Token-aware splitter
splitter = SentenceTransformersTokenTextSplitter(
    chunk_size=128,
    chunk_overlap=20,
    model_name="all-MiniLM-L6-v2"
)

# Parameters
MIN_CLUSTER_LEN = 3

# Phase-Level Collection Using Side Clustering
phase_blocks = []

for case_id, case_data in cases.items():
    utterances = case_data.get("convos", [])
    if not utterances:
        continue

    current_cluster = []
    current_side = None

    for utt in utterances:
        side = utt.get("side")
        text = utt.get("text", "").strip()
        if not text:
            continue

        if current_side is None:
            current_side = side

        if side != current_side and len(current_cluster) >= MIN_CLUSTER_LEN:
            full_text = " ".join(u["text"] for u in current_cluster)
            phase_blocks.append(Document(
                page_content=full_text,
                metadata={"case_id": case_id, "side": current_side}
            ))
            current_cluster = []
            current_side = side

        current_cluster.append(utt)

    if len(current_cluster) >= MIN_CLUSTER_LEN:
        full_text = " ".join(u["text"] for u in current_cluster)
        phase_blocks.append(Document(
            page_content=full_text,
            metadata={"case_id": case_id, "side": current_side}
        ))

# Token-aware semantic chunking using LangChain
def semantically_chunk_sentences(text):
    return splitter.split_text(text)

# Final chunking pass
convos_chunks = []

for doc in phase_blocks:
    sem_chunks = semantically_chunk_sentences(doc.page_content)
    for i, chunk in enumerate(sem_chunks):
        convos_chunks.append(Document(
            page_content=chunk.strip(),
            metadata={**doc.metadata, "semantic_chunk_id": i}
        ))

# Report
print(f"\nTotal transcript chunks: {len(convos_chunks)}")
if convos_chunks:
    print("Sample transcript chunk metadata:", convos_chunks[0].metadata)
    print("Sample chunk text snippet:", convos_chunks[0], "...")
else:
    print("⚠️ No transcript chunks were generated.")

with open("data/convos_chunks.pkl", "wb") as f:
    pickle.dump(convos_chunks, f)