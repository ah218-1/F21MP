# -*- coding: utf-8 -*-
"""FAISS Index.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YLq3cnZLqPbrC_6LHOdrQjEC27xF-rRI
"""

from langchain.schema import Document
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from tqdm import tqdm

import pickle

# Load chunks from previous step
with open("data/justia_chunks.pkl", "rb") as f:
    justia_chunks = pickle.load(f)

with open("data/convos_chunks.pkl", "rb") as f:
    convos_chunks = pickle.load(f)

# Merge chunked datasets
all_chunks = justia_chunks + convos_chunks  # Combine chunks
texts = [doc.page_content for doc in all_chunks]

with open("data/all_chunks.pkl", "wb") as f:
    pickle.dump(all_chunks, f)


# Load semantic model
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Embed in batches
batch_size = 256
embeddings = []

for i in tqdm(range(0, len(texts), batch_size), desc="Embedding batches"):
    batch = texts[i:i+batch_size]
    batch_embeddings = embedder.encode(batch)
    embeddings.extend(batch_embeddings)

# Convert to NumPy array
embeddings_np = np.array(embeddings)

# Build FAISS index
dimension = embeddings_np.shape[1]
index = faiss.IndexFlatL2(dimension)  # Euclidean distance
index.add(embeddings_np)

# Retain document references
indexed_docs = all_chunks

# Save FAISS index to disk
faiss.write_index(index, "data/justia_convos.index")
print(" FAISS index saved as justia_convos.index in data folder")

with open("data/indexed_docs.pkl", "wb") as f:
    pickle.dump(indexed_docs, f)

print("Pickled justia_chunks, convos_chunks, all_chunks, and indexed_docs to data/ folder")