{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRIc7P7ajLNT",
        "outputId": "e5f22d36-3be0-409b-b49a-1843f166a297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install transformers\n",
        "import json\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSONL dataset\n",
        "data_file = \"/content/super_scotus_sample.jsonl\"  # path to the dataset file\n",
        "cases = {}\n",
        "with open(data_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        case = json.loads(line)\n",
        "        case_id = case.get(\"case_id\") or case.get(\"id\")  # use appropriate ID field\n",
        "        # Separate Justia opinion sections and convos (transcript)\n",
        "        justia_sections = case.get(\"justia_sections\", {})  # e.g. {\"Syllabus\": \"...\", \"Opinion\": \"...\", ...}\n",
        "        convos = case.get(\"convos\", {})\n",
        "        cases[case_id] = {\n",
        "            \"justia_sections\": justia_sections,\n",
        "            \"convos\": convos\n",
        "        }\n",
        "\n",
        "print(f\"Loaded {len(cases)} cases from Super-SCOTUS dataset.\")\n",
        "# Example: Check structure for one case\n",
        "example_id, example_case = next(iter(cases.items()))\n",
        "print(\"Case ID:\", example_id)\n",
        "print(\"Justia sections:\", list(example_case[\"justia_sections\"].keys()))\n",
        "# If 'convos' is a dict of parallel lists (ids, speakers, text, etc.), convert to list of utterance dicts:\n",
        "convos = example_case[\"convos\"]\n",
        "if isinstance(convos, dict) and \"ids\" in convos:\n",
        "    utterances = []\n",
        "    ids = convos.get(\"ids\", [])\n",
        "    texts = convos.get(\"text\") or convos.get(\"texts\") or convos.get(\"utterances\")\n",
        "    speakers = convos.get(\"speakers\") or convos.get(\"speaker\")\n",
        "    sides = convos.get(\"sides\") or convos.get(\"side\")\n",
        "    speaker_types = convos.get(\"speaker_type\") or convos.get(\"speaker_types\")\n",
        "    # Combine parallel lists into structured utterances\n",
        "    for i, u_id in enumerate(ids):\n",
        "        utterance = {\n",
        "            \"id\": u_id,\n",
        "            \"text\": texts[i] if texts else \"\",\n",
        "            \"speaker\": speakers[i] if speakers else None,\n",
        "            \"side\": sides[i] if sides else None,\n",
        "            \"speaker_type\": speaker_types[i] if speaker_types else None\n",
        "        }\n",
        "        utterances.append(utterance)\n",
        "    cases[case_id][\"convos\"] = utterances\n",
        "elif isinstance(convos, list):\n",
        "    # Already a list of utterance dicts\n",
        "    cases[case_id][\"convos\"] = convos\n",
        "\n",
        "print(\"First 2 transcript utterances:\", cases[example_id][\"convos\"][:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "Sl7-oX46jL3L",
        "outputId": "2fbcbc8b-fbcf-4e4c-b505-f0cb8e6bdc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 cases from Super-SCOTUS dataset.\n",
            "Case ID: 1955_71\n",
            "Justia sections: ['Syllabus', 'Case']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-daeb939f5baf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mu_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;34m\"speaker\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspeakers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspeakers\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;34m\"side\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msides\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;34m\"speaker_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspeaker_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspeaker_types\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSONL dataset\n",
        "data_file = \"/content/super_scotus_sample_clean.jsonl\"  # path to the dataset file\n",
        "cases = {}\n",
        "\n",
        "with open(data_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        case = json.loads(line)\n",
        "        case_id = case.get(\"case_id\") or case.get(\"id\")  # use appropriate ID field\n",
        "        justia_sections = case.get(\"justia_sections\", {})  # e.g. {\"Syllabus\": \"...\", \"Opinion\": \"...\", ...}\n",
        "        convos = case.get(\"convos\", {})\n",
        "\n",
        "        # Begin parsing the convos\n",
        "        utterances = []\n",
        "\n",
        "        if isinstance(convos, dict) and \"ids\" in convos:\n",
        "            ids = convos.get(\"ids\", [])\n",
        "            texts = convos.get(\"text\") or convos.get(\"texts\") or []\n",
        "            speaker_map = convos.get(\"speaker\", {})  # This is a lookup dictionary\n",
        "            sides_map = speaker_map  # sides are stored inside speaker metadata\n",
        "            types_map = speaker_map  # same for types\n",
        "\n",
        "            for i, speaker_id in enumerate(ids):\n",
        "                u_text = texts[i] if i < len(texts) else \"\"\n",
        "                speaker_meta = speaker_map.get(speaker_id, {}) if speaker_id else {}\n",
        "\n",
        "                utterance = {\n",
        "                    \"id\": speaker_id,\n",
        "                    \"text\": u_text,\n",
        "                    \"speaker\": speaker_id,\n",
        "                    \"side\": speaker_meta.get(\"side\"),\n",
        "                    \"speaker_type\": speaker_meta.get(\"type\")\n",
        "                }\n",
        "                utterances.append(utterance)\n",
        "\n",
        "        elif isinstance(convos, list):\n",
        "            # Already a list of utterance dicts\n",
        "            utterances = convos\n",
        "\n",
        "        # Store the parsed case\n",
        "        cases[case_id] = {\n",
        "            \"justia_sections\": justia_sections,\n",
        "            \"convos\": utterances\n",
        "        }\n",
        "\n",
        "print(f\"Loaded {len(cases)} cases from Super-SCOTUS dataset.\")\n",
        "\n",
        "# Example: Check structure for one case\n",
        "example_id, example_case = next(iter(cases.items()))\n",
        "print(\"Case ID:\", example_id)\n",
        "print(\"Justia sections:\", list(example_case[\"justia_sections\"].keys()))\n",
        "print(\"First 2 transcript utterances:\", example_case[\"convos\"][:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUmwSUdyaljP",
        "outputId": "d3822448-556c-48cf-f3da-81a53a16a71a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 6733 cases from Super-SCOTUS dataset.\n",
            "Case ID: 1955_71\n",
            "Justia sections: ['Syllabus', 'Case']\n",
            "First 2 transcript utterances: [{'id': '13127', 'text': '', 'speaker': '13127', 'side': None, 'speaker_type': None}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSONL dataset\n",
        "data_file = \"/content/super_scotus_sample_clean.jsonl\"  # path to the dataset file\n",
        "cases = {}\n",
        "\n",
        "with open(data_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        case = json.loads(line)\n",
        "        case_id = case.get(\"case_id\") or case.get(\"id\")  # use appropriate ID field\n",
        "        justia_sections = case.get(\"justia_sections\", {})  # e.g. {\"Syllabus\": \"...\", \"Opinion\": \"...\", ...}\n",
        "        convos = case.get(\"convos\", {})\n",
        "\n",
        "        # Begin parsing the convos\n",
        "        utterances = []\n",
        "        speaker_map = convos.get(\"speaker\", {})  # speaker metadata (lookup by speaker_id)\n",
        "\n",
        "        # ✅ Case 1: Proper \"utterances\" field (nested list of dicts)\n",
        "        if isinstance(convos, dict) and \"utterances\" in convos:\n",
        "            outer = convos[\"utterances\"]\n",
        "            if (isinstance(outer, list) and len(outer) > 0 and isinstance(outer[0], list)\n",
        "                    and all(isinstance(u, dict) for u in outer[0])):\n",
        "\n",
        "                for utt in outer[0]:  # Flatten and parse\n",
        "                    speaker_id = utt.get(\"speaker_id\")\n",
        "                    speaker_meta = speaker_map.get(speaker_id, {}) if speaker_id else {}\n",
        "\n",
        "                    utterances.append({\n",
        "                        \"id\": utt.get(\"id\"),  # Some may not have an ID\n",
        "                        \"text\": utt.get(\"text\", \"\"),\n",
        "                        \"speaker\": speaker_id,\n",
        "                        \"side\": speaker_meta.get(\"side\"),\n",
        "                        \"speaker_type\": speaker_meta.get(\"type\")\n",
        "                    })\n",
        "            else:\n",
        "                print(f\"Warning: Unexpected 'utterances' format in case {case_id}. Skipping it.\")\n",
        "\n",
        "        # ✅ Case 2: Parallel list fallback\n",
        "        elif isinstance(convos, dict) and \"ids\" in convos:\n",
        "            ids = convos.get(\"ids\", [])\n",
        "            texts = convos.get(\"text\") or convos.get(\"texts\") or []\n",
        "\n",
        "            for i, speaker_id in enumerate(ids):\n",
        "                u_text = texts[i] if i < len(texts) else \"\"\n",
        "                speaker_meta = speaker_map.get(speaker_id, {}) if speaker_id else {}\n",
        "\n",
        "                utterances.append({\n",
        "                    \"id\": speaker_id,\n",
        "                    \"text\": u_text,\n",
        "                    \"speaker\": speaker_id,\n",
        "                    \"side\": speaker_meta.get(\"side\"),\n",
        "                    \"speaker_type\": speaker_meta.get(\"type\")\n",
        "                })\n",
        "\n",
        "        # ✅ Case 3: Already a flat list of utterance dicts\n",
        "        elif isinstance(convos, list):\n",
        "            utterances = convos\n",
        "\n",
        "        # Store the parsed case\n",
        "        cases[case_id] = {\n",
        "            \"justia_sections\": justia_sections,\n",
        "            \"convos\": utterances\n",
        "        }\n",
        "\n",
        "print(f\"Loaded {len(cases)} cases from Super-SCOTUS dataset.\")\n",
        "\n",
        "# Example: Check structure for one case\n",
        "example_id, example_case = next(iter(cases.items()))\n",
        "print(\"Case ID:\", example_id)\n",
        "print(\"Justia sections:\", list(example_case[\"justia_sections\"].keys()))\n",
        "print(\"First 2 transcript utterances:\", example_case[\"convos\"][:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aRkZAW1ztSX",
        "outputId": "1cb3d45d-f5c3-4f7a-d278-db34a63bfc06"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 6733 cases from Super-SCOTUS dataset.\n",
            "Case ID: 1955_71\n",
            "Justia sections: ['Syllabus', 'Case']\n",
            "First 2 transcript utterances: [{'id': None, 'text': 'Number 71, Lonnie Affronti versus United States of America.\\nMr. Murphy.', 'speaker': 'j__earl_warren', 'side': None, 'speaker_type': 'J'}, {'id': None, 'text': 'May it please the Court.\\nWe are here by writ of certiorari to the Eighth Circuit.\\nThere is one question to be decided in this case, decided carefully.\\nUpon sentence to consecutive sentences or terms by a District Court.\\nThe defending pattern started the service of a first sentence.\\nThus, the District Court thereafter have jurisdiction to suspend the execution of the remaining sentences and place the defendant on probation.', 'speaker': 'harry_f_murphy', 'side': 1, 'speaker_type': 'A'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSONL dataset\n",
        "data_file = \"/content/super_scotus_sample_clean.jsonl\"  # path to the dataset file\n",
        "cases = {}\n",
        "\n",
        "with open(data_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            case = json.loads(line)\n",
        "        except json.JSONDecodeError as e:\n",
        "                print(f\"Skipping malformed line: {e}\")\n",
        "                continue\n",
        "        case_id = case.get(\"case_id\") or case.get(\"id\")  # use appropriate ID field\n",
        "        justia_sections = case.get(\"justia_sections\", {})  # e.g. {\"Syllabus\": \"...\", \"Opinion\": \"...\", ...}\n",
        "        convos = case.get(\"convos\", {})\n",
        "\n",
        "        # Begin parsing the convos\n",
        "        utterances = []\n",
        "        speaker_map = convos.get(\"speaker\", {})  # speaker metadata (lookup by speaker_id)\n",
        "\n",
        "        # ✅ Case 1: Proper \"utterances\" field (nested list of dicts)\n",
        "        if isinstance(convos, dict) and \"utterances\" in convos:\n",
        "            outer = convos[\"utterances\"]\n",
        "            if (isinstance(outer, list) and len(outer) > 0 and isinstance(outer[0], list)\n",
        "                    and all(isinstance(u, dict) for u in outer[0])):\n",
        "\n",
        "                for utt in outer[0]:  # Flatten and parse\n",
        "                    utt_id = utt.get(\"id\")\n",
        "                    speaker_id = utt.get(\"speaker_id\")\n",
        "                    text = utt.get(\"text\", \"\")\n",
        "\n",
        "                    speaker_meta = speaker_map.get(speaker_id, {}) if speaker_id else {}\n",
        "                    speaker_type = speaker_meta.get(\"type\")\n",
        "\n",
        "                    # Debug this\n",
        "                    if speaker_type is None:\n",
        "                        print(f\"[DEBUG] Missing speaker_type for: speaker_id={speaker_id}, utt_text={text[:50]}\")\n",
        "\n",
        "                    utterances.append({\n",
        "                         \"id\": utt_id,\n",
        "                         \"text\": text,\n",
        "                         \"speaker\": speaker_id,\n",
        "                         \"side\": speaker_meta.get(\"side\"),\n",
        "                         \"speaker_type\": speaker_type\n",
        "                    })\n",
        "\n",
        "            else:\n",
        "                print(f\"Warning: Unexpected 'utterances' format in case {case_id}. Skipping it.\")\n",
        "\n",
        "        # ✅ Case 2: Parallel list fallback\n",
        "        elif isinstance(convos, dict) and \"ids\" in convos:\n",
        "            ids = convos.get(\"ids\", [])\n",
        "            texts = convos.get(\"text\") or convos.get(\"texts\") or []\n",
        "\n",
        "            for i, speaker_id in enumerate(ids):\n",
        "                u_text = texts[i] if i < len(texts) else \"\"\n",
        "                speaker_meta = speaker_map.get(speaker_id, {}) if speaker_id else {}\n",
        "\n",
        "                utterances.append({\n",
        "                    \"id\": speaker_id,\n",
        "                    \"text\": u_text,\n",
        "                    \"speaker\": speaker_id,\n",
        "                    \"side\": speaker_meta.get(\"side\"),\n",
        "                    \"speaker_type\": speaker_meta.get(\"type\")\n",
        "                })\n",
        "\n",
        "        # ✅ Case 3: Already a flat list of utterance dicts\n",
        "        elif isinstance(convos, list):\n",
        "            utterances = convos\n",
        "\n",
        "        # Store the parsed case\n",
        "        cases[case_id] = {\n",
        "            \"justia_sections\": justia_sections,\n",
        "            \"convos\": utterances\n",
        "        }\n",
        "\n",
        "print(f\"Loaded {len(cases)} cases from Super-SCOTUS dataset.\")\n",
        "\n",
        "# Example: Check structure for one case\n",
        "example_id, example_case = next(iter(cases.items()))\n",
        "print(\"Case ID:\", example_id)\n",
        "print(\"Justia sections:\", list(example_case[\"justia_sections\"].keys()))\n",
        "print(\"First 2 transcript utterances:\", example_case[\"convos\"][:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e990e93-2313-4320-86bb-243a9d136d74",
        "id": "SjAF8TeXN0bv"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 6733 cases from Super-SCOTUS dataset.\n",
            "Case ID: 1955_71\n",
            "Justia sections: ['Syllabus', 'Case']\n",
            "First 2 transcript utterances: [{'id': None, 'text': 'Number 71, Lonnie Affronti versus United States of America.\\nMr. Murphy.', 'speaker': 'j__earl_warren', 'side': None, 'speaker_type': 'J'}, {'id': None, 'text': 'May it please the Court.\\nWe are here by writ of certiorari to the Eighth Circuit.\\nThere is one question to be decided in this case, decided carefully.\\nUpon sentence to consecutive sentences or terms by a District Court.\\nThe defending pattern started the service of a first sentence.\\nThus, the District Court thereafter have jurisdiction to suspend the execution of the remaining sentences and place the defendant on probation.', 'speaker': 'harry_f_murphy', 'side': 1, 'speaker_type': 'A'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain tiktoken  # ensure LangChain is installed for text splitting\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize a text splitter (e.g., aiming for ~300 tokens, with some overlap)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)  # chunk_size in characters (approx tokens)\n",
        "\n",
        "justia_chunks = []  # will hold LangChain Document objects for each chunk\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "for case_id, case_data in cases.items():\n",
        "    sections = case_data[\"justia_sections\"]\n",
        "    if not sections:\n",
        "        continue  # skip if no Justia text\n",
        "    for section_name, section_text in sections.items():\n",
        "        # Determine section type and justice (if section_name contains a Justice's name for concurrences/dissents)\n",
        "        sec_type = section_name\n",
        "        justice = None\n",
        "        # Example: section_name might be \"Opinion\" or \"Opinion of Justice X\", or \"Dissenting - Justice Y\"\n",
        "        if \"Concurring\" in section_name or \"Dissent\" in section_name:\n",
        "            sec_type = \"Concurring\" if \"Concurring\" in section_name else \"Dissenting\"\n",
        "            # Extract justice name if present\n",
        "            # e.g. \"Concurring Opinion by Justice Doe\" or \"Justice Doe, dissenting\"\n",
        "            words = section_name.split()\n",
        "            for w in words:\n",
        "                if w.capitalize() == \"Justice\" or w.endswith(\"J.\") or w.endswith(\"C.J.\"):\n",
        "                    # The next word might be last name of Justice\n",
        "                    # Simple heuristic: find index of \"Justice\" and take next word\n",
        "                    try:\n",
        "                        idx = words.index(w)\n",
        "                        justice = words[idx+1].strip(\",\")\n",
        "                    except:\n",
        "                        justice = None\n",
        "            # If justice still None, it might be something like \"Justice Doe, dissenting\"\n",
        "            if justice is None and \"Justice\" in section_text:\n",
        "                # fallback: find first occurrence of \"Justice\" in text\n",
        "                start = section_text.find(\"Justice\")\n",
        "                if start != -1:\n",
        "                    justice = section_text[start:section_text.find(\" \", start+8)]\n",
        "        elif \"Opinion\" in section_name and \"Justice\" in section_name:\n",
        "            sec_type = \"Opinion\"\n",
        "            # e.g. \"Opinion of Justice X\"\n",
        "            try:\n",
        "                justice_idx = section_name.split().index(\"Justice\")\n",
        "                justice = section_name.split()[justice_idx+1]\n",
        "            except:\n",
        "                justice = None\n",
        "        else:\n",
        "            # e.g. \"Syllabus\" or generic \"Opinion\"\n",
        "            sec_type = section_name\n",
        "\n",
        "        # Split section text into chunks\n",
        "        for chunk in text_splitter.split_text(section_text):\n",
        "            doc = Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"case_id\": case_id,\n",
        "                    \"section_type\": sec_type,\n",
        "                    \"justice\": justice\n",
        "                }\n",
        "            )\n",
        "            justia_chunks.append(doc)\n",
        "\n",
        "print(f\"Total Justia chunks: {len(justia_chunks)} (from {len(cases)} cases)\")\n",
        "print(\"Sample chunk metadata:\", justia_chunks[0].metadata)\n",
        "print(\"Sample chunk text snippet:\", justia_chunks[0].page_content[:100], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v0AuiBFSYe4",
        "outputId": "b44587ac-6672-4b2d-95b6-c13f0ea612e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Total Justia chunks: 353627 (from 6733 cases)\n",
            "Sample chunk metadata: {'case_id': '1955_71', 'section_type': 'Syllabus', 'justice': None}\n",
            "Sample chunk text snippet: U.S. Supreme CourtAffronti v. United States, 350 U.S. 79 (1955)Affronti v. United StatesNo. 71Argued ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "convos_chunks = []  # list of Document chunks for transcripts\n",
        "\n",
        "for case_id, case_data in cases.items():\n",
        "    utterances = case_data.get(\"convos\")\n",
        "    if not utterances:\n",
        "        continue\n",
        "    # Determine phase order by looking at sides of advocate speakers\n",
        "    current_phase = None  # \"Petitioner\", \"Respondent\", or \"Rebuttal\"\n",
        "    current_side = None   # \"petitioner\" or \"respondent\" (from data)\n",
        "    current_chunk_text = \"\"\n",
        "    current_chunk_utts = []\n",
        "\n",
        "    for utt in utterances:\n",
        "        speaker_type = utt.get(\"speaker_type\")\n",
        "        side = utt.get(\"side\")  # e.g., \"petitioner\" or \"respondent\" or None for justices\n",
        "        text = utt.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue  # skip empty utterances if any\n",
        "\n",
        "        # Determine if a phase switch should occur\n",
        "        if speaker_type == \"A\" or speaker_type == \"Advocate\":  # an attorney (advocate) turn\n",
        "            # Map side to phase name (capitalize first letter)\n",
        "            if side:\n",
        "                side_name = side.capitalize()  # \"Petitioner\" or \"Respondent\"\n",
        "            else:\n",
        "                # If side not given, infer from context or speaker name (not covered here)\n",
        "                side_name = current_phase or \"Petitioner\"\n",
        "        else:\n",
        "            # A Justice speaking does not change the phase; they are asking questions.\n",
        "            side_name = current_phase\n",
        "\n",
        "        # Initialize phase if not set\n",
        "        if current_phase is None:\n",
        "            current_phase = side_name\n",
        "\n",
        "        # If the side/phase has changed (from Petitioner to Respondent or vice versa),\n",
        "        # or if we explicitly detect a \"Rebuttal\" (Petitioner speaking again after Respondent):\n",
        "        if side_name and current_phase and side_name != current_phase:\n",
        "            # Finish the current phase chunk (if any text accumulated)\n",
        "            if current_chunk_text:\n",
        "                convos_chunks.append(Document(\n",
        "                    page_content=current_chunk_text.strip(),\n",
        "                    metadata={\n",
        "                        \"case_id\": case_id,\n",
        "                        \"phase\": current_phase,\n",
        "                        \"utterance_ids\": current_chunk_utts\n",
        "                    }\n",
        "                ))\n",
        "            # Reset for new phase\n",
        "            current_phase = side_name\n",
        "            current_chunk_text = \"\"\n",
        "            current_chunk_utts = []\n",
        "\n",
        "        # Add the current utterance to the chunk\n",
        "        # Check if adding this utterance would exceed token limit (~300-500 tokens)\n",
        "        tokens = word_tokenize(text)\n",
        "        if len(word_tokenize(current_chunk_text)) + len(tokens) > 400:\n",
        "            # If current chunk is already non-empty, close it and start a new chunk\n",
        "            if current_chunk_text:\n",
        "                convos_chunks.append(Document(\n",
        "                    page_content=current_chunk_text.strip(),\n",
        "                    metadata={\n",
        "                        \"case_id\": case_id,\n",
        "                        \"phase\": current_phase,\n",
        "                        \"utterance_ids\": current_chunk_utts\n",
        "                    }\n",
        "                ))\n",
        "            # Reset chunk content\n",
        "            current_chunk_text = \"\"\n",
        "            current_chunk_utts = []\n",
        "        # Append this utterance\n",
        "        current_chunk_text += \" \" + text\n",
        "        current_chunk_utts.append(utt.get(\"id\"))\n",
        "\n",
        "    # Add the final chunk for this case and phase if exists\n",
        "    if current_chunk_text:\n",
        "        convos_chunks.append(Document(\n",
        "            page_content=current_chunk_text.strip(),\n",
        "            metadata={\n",
        "                \"case_id\": case_id,\n",
        "                \"phase\": current_phase,\n",
        "                \"utterance_ids\": current_chunk_utts\n",
        "            }\n",
        "        ))\n",
        "\n",
        "print(f\"Total transcript chunks: {len(convos_chunks)}\")\n",
        "print(\"Sample transcript chunk metadata:\", convos_chunks[0].metadata)\n",
        "print(\"Sample chunk text snippet:\", convos_chunks[0].page_content[:100], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "2ExcgzzKSvuX",
        "outputId": "5fd8bbf2-4580-438d-91ba-fb0c64dcc1b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-681fac096f38>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Add the current utterance to the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Check if adding this utterance would exceed token limit (~300-500 tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_chunk_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# If current chunk is already non-empty, close it and start a new chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from langchain.schema import Document  # Ensure this import if not already included\n",
        "\n",
        "# You can also debug available speaker types with:\n",
        "speaker_types_seen = set()\n",
        "for case in cases.values():\n",
        "    for utt in case.get(\"convos\", []):\n",
        "        if utt.get(\"speaker_type\"):\n",
        "            speaker_types_seen.add(utt[\"speaker_type\"].lower())\n",
        "\n",
        "print(\"Detected speaker types in dataset:\", speaker_types_seen)\n",
        "\n",
        "# Define which speaker types should be treated as 'advocates'\n",
        "ADVOCATE_TYPES = {\"a\", \"advocate\", \"attorney\", \"counsel\"}\n",
        "\n",
        "convos_chunks = []  # list of Document chunks for transcripts\n",
        "\n",
        "for case_id, case_data in cases.items():\n",
        "    utterances = case_data.get(\"convos\")\n",
        "    if not utterances:\n",
        "        continue\n",
        "\n",
        "    current_phase = None  # \"Petitioner\", \"Respondent\", or \"Rebuttal\"\n",
        "    current_chunk_text = \"\"\n",
        "    current_chunk_utts = []\n",
        "\n",
        "    for utt in utterances:\n",
        "        speaker_type = utt.get(\"speaker_type\")\n",
        "        side = utt.get(\"side\")  # e.g., \"petitioner\", \"respondent\", or None\n",
        "        text = utt.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue  # skip empty utterances\n",
        "\n",
        "        # Normalize speaker_type string\n",
        "        speaker_type_normalized = speaker_type.lower() if speaker_type else None\n",
        "\n",
        "        # Determine phase from speaker_type and side\n",
        "        if speaker_type_normalized in ADVOCATE_TYPES:\n",
        "            side_name = side.capitalize() if side else (current_phase or \"Petitioner\")\n",
        "        else:\n",
        "            side_name = current_phase  # Justices don't shift phase\n",
        "\n",
        "        # Initialize current phase if needed\n",
        "        if current_phase is None:\n",
        "            current_phase = side_name\n",
        "\n",
        "        # Detect phase switch\n",
        "        if side_name and current_phase and side_name != current_phase:\n",
        "            if current_chunk_text:\n",
        "                convos_chunks.append(Document(\n",
        "                    page_content=current_chunk_text.strip(),\n",
        "                    metadata={\n",
        "                        \"case_id\": case_id,\n",
        "                        \"phase\": current_phase,\n",
        "                        \"utterance_ids\": current_chunk_utts\n",
        "                    }\n",
        "                ))\n",
        "            # Reset for new phase\n",
        "            current_phase = side_name\n",
        "            current_chunk_text = \"\"\n",
        "            current_chunk_utts = []\n",
        "\n",
        "        # Estimate tokens and split if needed\n",
        "        tokens = word_tokenize(text)\n",
        "        if len(word_tokenize(current_chunk_text)) + len(tokens) > 400:\n",
        "            if current_chunk_text:\n",
        "                convos_chunks.append(Document(\n",
        "                    page_content=current_chunk_text.strip(),\n",
        "                    metadata={\n",
        "                        \"case_id\": case_id,\n",
        "                        \"phase\": current_phase,\n",
        "                        \"utterance_ids\": current_chunk_utts\n",
        "                    }\n",
        "                ))\n",
        "            current_chunk_text = \"\"\n",
        "            current_chunk_utts = []\n",
        "\n",
        "        # Accumulate this utterance\n",
        "        current_chunk_text += \" \" + text\n",
        "        current_chunk_utts.append(utt.get(\"id\"))\n",
        "\n",
        "    # Add final chunk if it exists\n",
        "    if current_chunk_text:\n",
        "        convos_chunks.append(Document(\n",
        "            page_content=current_chunk_text.strip(),\n",
        "            metadata={\n",
        "                \"case_id\": case_id,\n",
        "                \"phase\": current_phase,\n",
        "                \"utterance_ids\": current_chunk_utts\n",
        "            }\n",
        "        ))\n",
        "\n",
        "# ✅ Safe printing\n",
        "print(f\"\\nTotal transcript chunks: {len(convos_chunks)}\")\n",
        "if convos_chunks:\n",
        "    print(\"Sample transcript chunk metadata:\", convos_chunks[0].metadata)\n",
        "    print(\"Sample chunk text snippet:\", convos_chunks[0].page_content[:100], \"...\")\n",
        "else:\n",
        "    print(\"⚠️ No transcript chunks were generated. Check if any utterances matched speaker_type filters.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "m8F7D7MZdPH2",
        "outputId": "48668847-a393-4a79-af0a-73fa21422017"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected speaker types in dataset: {'u', 'j', 'a'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7953cba12892>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Estimate tokens and split if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_chunk_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_chunk_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure NLTK is installed\n",
        "!pip install nltk --quiet\n",
        "\n",
        "import nltk\n",
        "import shutil\n",
        "from nltk.tokenize import word_tokenize\n",
        "from langchain.schema import Document  # Ensure this import if not already included\n",
        "\n",
        "# 🔁 Step 1: Clean and redownload tokenizer data to fix punkt_tab issue\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True)  # Optional but ensures clean state\n",
        "nltk.download('punkt')  # Redownload tokenizer model\n",
        "\n",
        "# 🔍 Debug available speaker types\n",
        "speaker_types_seen = set()\n",
        "for case in cases.values():\n",
        "    for utt in case.get(\"convos\", []):\n",
        "        if utt.get(\"speaker_type\"):\n",
        "            speaker_types_seen.add(utt[\"speaker_type\"].lower())\n",
        "\n",
        "print(\"Detected speaker types in dataset:\", speaker_types_seen)\n",
        "\n",
        "# ⚖️ Define which speaker types are considered 'advocates'\n",
        "ADVOCATE_TYPES = {\"a\", \"advocate\", \"attorney\", \"counsel\"}\n",
        "\n",
        "# 📄 Chunk container\n",
        "convos_chunks = []\n",
        "\n",
        "for case_id, case_data in cases.items():\n",
        "    utterances = case_data.get(\"convos\")\n",
        "    if not utterances:\n",
        "        continue\n",
        "\n",
        "    current_phase = None  # e.g., \"Petitioner\", \"Respondent\"\n",
        "    current_chunk_text = \"\"\n",
        "    current_chunk_utts = []\n",
        "\n",
        "    for utt in utterances:\n",
        "        speaker_type = utt.get(\"speaker_type\")\n",
        "        side = utt.get(\"side\")\n",
        "        text = utt.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Normalize speaker_type\n",
        "        speaker_type_normalized = speaker_type.lower() if speaker_type else None\n",
        "\n",
        "        # Determine phase based on advocate role\n",
        "        if speaker_type_normalized in ADVOCATE_TYPES:\n",
        "            side_name = side.capitalize() if side else (current_phase or \"Petitioner\")\n",
        "        else:\n",
        "            side_name = current_phase\n",
        "\n",
        "        # Initialize phase if not already set\n",
        "        if current_phase is None:\n",
        "            current_phase = side_name\n",
        "\n",
        "        # Detect phase change and split\n",
        "        if side_name and current_phase and side_name != current_phase:\n",
        "            if current_chunk_text:\n",
        "                convos_chunks.append(Document(\n",
        "                    page_content=current_chunk_text.strip(),\n",
        "                    metadata={\n",
        "                        \"case_id\": case_id,\n",
        "                        \"phase\": current_phase,\n",
        "                        \"utterance_ids\": current_chunk_utts\n",
        "                    }\n",
        "                ))\n",
        "            current_phase = side_name\n",
        "            current_chunk_text = \"\"\n",
        "            current_chunk_utts = []\n",
        "\n",
        "        # Estimate tokens and split if needed\n",
        "        tokens = word_tokenize(text)\n",
        "        if len(word_tokenize(current_chunk_text)) + len(tokens) > 400:\n",
        "            if current_chunk_text:\n",
        "                convos_chunks.append(Document(\n",
        "                    page_content=current_chunk_text.strip(),\n",
        "                    metadata={\n",
        "                        \"case_id\": case_id,\n",
        "                        \"phase\": current_phase,\n",
        "                        \"utterance_ids\": current_chunk_utts\n",
        "                    }\n",
        "                ))\n",
        "            current_chunk_text = \"\"\n",
        "            current_chunk_utts = []\n",
        "\n",
        "        # Add utterance to chunk\n",
        "        current_chunk_text += \" \" + text\n",
        "        current_chunk_utts.append(utt.get(\"id\"))\n",
        "\n",
        "    # Final chunk flush\n",
        "    if current_chunk_text:\n",
        "        convos_chunks.append(Document(\n",
        "            page_content=current_chunk_text.strip(),\n",
        "            metadata={\n",
        "                \"case_id\": case_id,\n",
        "                \"phase\": current_phase,\n",
        "                \"utterance_ids\": current_chunk_utts\n",
        "            }\n",
        "        ))\n",
        "\n",
        "# ✅ Report\n",
        "print(f\"\\nTotal transcript chunks: {len(convos_chunks)}\")\n",
        "if convos_chunks:\n",
        "    print(\"Sample transcript chunk metadata:\", convos_chunks[0].metadata)\n",
        "    print(\"Sample chunk text snippet:\", convos_chunks[0].page_content[:100], \"...\")\n",
        "else:\n",
        "    print(\"⚠️ No transcript chunks were generated. Check if any utterances matched speaker_type filters.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "M7xAM2Ys_YMv",
        "outputId": "8a2f7adc-a9a2-40f4-d071-0f716b0ddc54"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected speaker types in dataset: {'u', 'j', 'a'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e4c33d94afbb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Estimate tokens and split if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_chunk_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_chunk_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup ---\n",
        "!pip install nltk langchain --quiet\n",
        "\n",
        "import nltk\n",
        "import shutil\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# 🔁 Ensure clean download of tokenizer models\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# --- Parameters ---\n",
        "ADVOCATE_TYPES = {\"a\", \"advocate\", \"attorney\", \"counsel\"}\n",
        "MAX_CHUNK_SIZE = 1000  # characters (can also use token-based splitter)\n",
        "\n",
        "# --- LangChain's TextSplitter ---\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=MAX_CHUNK_SIZE,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# --- Phase-Level Collection ---\n",
        "phase_blocks = []\n",
        "\n",
        "for case_id, case_data in cases.items():\n",
        "    utterances = case_data.get(\"convos\")\n",
        "    if not utterances:\n",
        "        continue\n",
        "\n",
        "    current_phase = None\n",
        "    current_text = \"\"\n",
        "    current_utts = []\n",
        "\n",
        "    for utt in utterances:\n",
        "        speaker_type = utt.get(\"speaker_type\", \"\").lower()\n",
        "        side = utt.get(\"side\")\n",
        "        text = utt.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        side_name = str(side).capitalize() if speaker_type in ADVOCATE_TYPES and side is not None else current_phase\n",
        "\n",
        "\n",
        "        if current_phase is None:\n",
        "            current_phase = side_name\n",
        "\n",
        "        # Phase shift\n",
        "        if side_name != current_phase and current_text:\n",
        "            phase_blocks.append(Document(\n",
        "                page_content=current_text.strip(),\n",
        "                metadata={\"case_id\": case_id, \"phase\": current_phase, \"utterance_ids\": current_utts}\n",
        "            ))\n",
        "            current_phase = side_name\n",
        "            current_text = \"\"\n",
        "            current_utts = []\n",
        "\n",
        "        current_text += \" \" + text\n",
        "        current_utts.append(utt.get(\"id\"))\n",
        "\n",
        "    if current_text:\n",
        "        phase_blocks.append(Document(\n",
        "            page_content=current_text.strip(),\n",
        "            metadata={\"case_id\": case_id, \"phase\": current_phase, \"utterance_ids\": current_utts}\n",
        "        ))\n",
        "\n",
        "# --- Now use LangChain to split all phase blocks into chunks ---\n",
        "convos_chunks = []\n",
        "for doc in phase_blocks:\n",
        "    chunks = splitter.split_documents([doc])\n",
        "    convos_chunks.extend(chunks)\n",
        "\n",
        "# --- Report ---\n",
        "print(f\"\\nTotal transcript chunks: {len(convos_chunks)}\")\n",
        "if convos_chunks:\n",
        "    print(\"Sample transcript chunk metadata:\", convos_chunks[0].metadata)\n",
        "    print(\"Sample chunk text snippet:\", convos_chunks[0].page_content[:100], \"...\")\n",
        "else:\n",
        "    print(\"⚠️ No transcript chunks were generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qra1FVtRBkKe",
        "outputId": "878ceea5-0eb0-47de-9325-e48c23d29339"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total transcript chunks: 443294\n",
            "Sample transcript chunk metadata: {'case_id': '1955_71', 'phase': '1', 'utterance_ids': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]}\n",
            "Sample chunk text snippet: Number 71, Lonnie Affronti versus United States of America.\n",
            "Mr. Murphy. May it please the Court.\n",
            "We  ...\n"
          ]
        }
      ]
    }
  ]
}