# -*- coding: utf-8 -*-
"""RAG and Eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qb5-8kFw93Ke8tTByL6VdjivLFyWnWpL
"""

# --- ðŸ“š Imports ---
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from rouge_score import rouge_scorer
import numpy as np
import pandas as pd
import pickle
import faiss
from qafacteval import QAFactEval
import torch

# Load persisted variables
with open("data/gold_labels.pkl", "rb") as f:
    gold_labels = pickle.load(f)

with open("data/justia_chunks.pkl", "rb") as f:
    justia_chunks = pickle.load(f)

with open("data/convos_chunks.pkl", "rb") as f:
    convos_chunks = pickle.load(f)

with open("data/indexed_docs.pkl", "rb") as f:
    indexed_docs = pickle.load(f)

embeddings_np = np.load("data/embeddings.npy")
index = faiss.read_index("data/justia_convos.index")

# Load LexT5
tokenizer = AutoTokenizer.from_pretrained("lexlms/lex-t5-small")
model = AutoModelForSeq2SeqLM.from_pretrained("lexlms/lex-t5-small")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Load QAFactEval
metric = QAFactEval(
    lerc_quip_path="models/quip-512-mocha",
    generation_model_path="models/generation/model.tar.gz",
    answering_model_dir="models/answering",
    lerc_model_path="models/lerc/model.tar.gz",
    lerc_pretrained_model_path="models/lerc/pretraining.tar.gz",
    cuda_device=0,
    verbose=False
)

# Initialize ROUGE scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

# Retrieval function
TOP_K = 3

def retrieve_justia_chunks(query_vector, case_id, top_k=TOP_K):
    distances, indices = index.search(query_vector, 50)
    justia_indices = [idx for idx in indices[0] if idx < len(justia_chunks)
                      and indexed_docs[idx].metadata.get("case_id") == case_id]
    justia_indices = justia_indices[:top_k]
    return [indexed_docs[idx] for idx in justia_indices]

# Main pipeline
results = []
case_ids = sorted({doc.metadata["case_id"] for doc in convos_chunks})

for case_id in case_ids:
    transcripts_text = " ".join(
        [doc.page_content for doc in convos_chunks if doc.metadata.get("case_id") == case_id]
    )
    if not transcripts_text:
        continue

    justia_text = " ".join(
        [doc.page_content for doc in justia_chunks if doc.metadata.get("case_id") == case_id]
    )

    convos_idxs = [i for i, doc in enumerate(indexed_docs)
                   if doc.metadata.get("case_id") == case_id and i >= len(justia_chunks)]
    if not convos_idxs:
        continue

    query_vector = np.mean(embeddings_np[convos_idxs], axis=0).astype('float32').reshape(1, -1)
    support_docs = retrieve_justia_chunks(query_vector, case_id)

    references_text = "\n".join(
        [f"[{j+1}] {doc.page_content}" for j, doc in enumerate(support_docs)]
    )

    full_source_text = transcripts_text + "\n\n" + justia_text

    prompt = (
        f"summarize: Given the following oral argument transcript for case {case_id}, "
        f"create a factual summary grounded in the references.\n\n"
        f"Transcript:\n{transcripts_text}\n\n"
        f"Cite the relevant legal opinion excerpts provided below to support the summary.\n"
        f"References (legal opinion excerpts):\n{references_text}\n\n"
        f"Write the summary with numbered citations where relevant."
    )

    # Use LexT5 for generation
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)

    outputs = model.generate(
        **inputs,
        max_length=512,
        num_beams=4,
        early_stopping=True
    )

    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Case {case_id} Summary:\n{summary}")

    # QAFactEval evaluation
    qa_score = metric.score_batch(
        [full_source_text],
        [[summary]]
    )[0][0]['qa-eval']['lerc_quip']
    print(f"Consistency Score (QAFactEval): {qa_score:.2f}")

    # Compute ROUGE if gold label exists
    if case_id in gold_labels:
        ref = gold_labels[case_id]
        rouge_score = scorer.score(ref, summary)
        print(f"ROUGE for Case {case_id}:", rouge_score)
    else:
        rouge_score = None

    results.append({
        "case_id": case_id,
        "summary": summary,
        "consistency_score": qa_score,
        "rouge_score": rouge_score
    })

# Save results
pd.DataFrame(results).to_csv("data/summaries_and_scores.csv", index=False)